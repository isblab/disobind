######## Create a non-redundant train and OOD dataset ########
####### ------>"May the Force serve u well..." <------########
##############################################################


############# One above all #############
##-------------------------------------##
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import os
import argparse
import subprocess
import glob
import time
import h5py
from multiprocessing import Pool
from functools import partial
import tqdm
import json

from utility import ( mmseqs_cluster, read_mmseqs_tsv_output,
					usalign, get_aligned_TM_score )

import warnings
warnings.filterwarnings("ignore")

np.random.seed( 11 )

class NonRedundantDataset():
	def __init__ ( self, version, cores ):
		"""
		Constructor
		"""
		self.tim = time.time()   # Just for calculating total time.
		self.version = version
		self.base_path = f"../Database/{self.version}"
		self.dataset_version = "100_20_0.2"
		# Dir containing PDB/CIF files.
		self.pdb_path = "../Combined_PDBs/"
		# Dir containing PDB70 files.
		self.pdb70_path = "../PDB70/"
		# JSON dict storing Uniprot sequences.
		self.uni_seq_path = f"../Disobind_dataset_{self.dataset_version}/Uniprot_seq.json"
		# Dir to store the merged binary complexes.
		self.merged_binary_complexes_dir = "./merged_binary_complexes/"
		# Fiel to store keys for merged binary complexes.
		self.merged_binary_complexes_file = "./merged_binary_complexes.txt"
		# File to store all merged PDBs that are part of the binary complexes.
		self.selected_merged_pdbs_file = "./Merged_PDBs.txt"
		self.difference_set_file = "./difference_set.txt"
		self.ood_singleton_clusters_file = "./ood_singleton_clusters.txt"
		# self.train_singleton_clusters_file = "./train_singleton_clusters.txt"

		# AF2 related dir/files.
		# Dir to store FASTA files with prot1/2 sequence for AF2.
		self.af2_fasta_dir = "./AF2_fasta_dir/"
		# File containing comma-separated paths to all fasta files for AF2.
		self.af2_input_file = "./AF2_input.txt"

		# AF3 related dir/files.
		self.af3_json_dir = "./AF3_json_dir/"

		# PDB70 clusters tsv file.
		self.pdb70_tsv_file = f"../input_files/pdb70_clu.tsv"

		#### MMSeqs2 related dir/files.
		# Files for running MMSeqs2 to obtain OOD set.
		# ------------------------------------
		# Name for output files generated by MMSeqs2 for full dataset.
		self.name2 = "ood"
		# FASTA file for PDB70 sequences.
		self.pdb70_rep_fasta_file = f"../input_files/pdb70_rep_fasta_file.txt"
		# Dir for saving MMSeqs2 output files for ood dataset.
		self.ood_mm_dir = f"./mmseqs_{self.name2}/"
		# FASTA file to store sequence for input to MMSeqs2 for disobind and PDB70 comparison.
		self.ood_fasta_file = f"./{self.name2}_fasta.fasta"
		# Path to tsv file generated by MMSeqs2 for full dataset.
		self.ood_tsv_file = f"{self.ood_mm_dir}{self.name2}_cluster.tsv"
		# File to store MMSeqs2 clusters without any containing PDB70 chains.
		self.non_pdb70_clusters_file = f"{self.ood_mm_dir}non_redundant_clusters.csv"

		# Files for running MMSeqs2 for train set.
		# ------------------------------------		
		# Name for output files generated by MMSeqs2 for full dataset.
		self.name3 = "train"
		# Dir for saving MMSeqs2 output files for train dataset only.
		self.train_mm_dir = f"./mmseqs_{self.name3}/"
		# FASTA file to store sequence for input to MMSeqs2 for train dataset.
		self.train_fasta_file = f"./{self.name3}_fasta.fasta"
		# Path to tsv file generated by MMSeqs2 for train dataset.
		self.train_tsv_file = f"{self.train_mm_dir}{self.name3}_cluster.tsv"
		
		#### USalign related dir/files.
		# Path to the USalign executable.
		self.usalign_script = "/home/kartik/Disorder_Proteins/dataset/USalign"
		# self.usalign_script = "/home/kartik/Documents/Disorder_Proteins/disobind/dataset/USalign"
		# Dir to store USalign tmp files.
		self.usalign_dir = "./USalign/"
		
		# File to store accessory info for each merged binary complex.
		self.accessory_file = "./Accessory.json"
		# File to store all non-redundant binary complexe keys obtained.
		self.nr_binary_complexes_file = "./nr_binary_complexes.txt"
		# File to store test set keys.
		self.test_keys_file = "./test_binary_complexes.txt"
		self.train_keys_file = "./train_binary_complexes.txt"

		# No. of cores to parallelize the tasks.
		self.cores = cores
		# min-max bounds for the no. of contacts.
		self.contact_range = [0.005, 0.05] 
		# Check nonredunadncy for both proteins while creating OOD set.
		# self.redundancyreduce_both = True
		# Fraction of ODO keys to be consider for test set.
		self.ood_fraction = 0.01
		# sequence identity cutoff for ood set.
		self.ood_ID_cutoff = 0.2
		# Redundancy reduce the training set.
		self.redundancyreduce_train = False
		# sequence identity cutoff for training set.
		self.train_ID_cutoff = 0.4
		self.mmseqs_algo = "easy-cluster"# either of ["easy-linclust", "easy-cluster"]
		self.mmseqs_cluster_mode = 1     # Algorithm used for clustering [0, 1, 2].

		# Set to store all merged PDBs.
		self.unique_merged_pdbs = set()
		# JSON dict to store accessory info for each merged binary complex.
		# self.accessory = {}
		# List to store all OOD keys obtained.
		self.ood_keys = []
		# dict for to store sequence, cmap for full dataset.
		self.entry_dict = {}
		# dict for to store sequence, cmap for train dataset.
		self.train_dict = {}
		# dict for to store sequence, cmap for OOD test set.
		self.test_dict = {}
		
		# JSON dict storing the logs.
		self.logger_file = f"./Logs_OOD_{self.version}.json"
		self.logger = {"time_taken": {}, "counts": {}}


	def forward( self ):
		"""
		Using the same directory structure as created by
			create_merged_binary_complexes.py.
		Raise an error if the directory does not exist.
		"""
		if not os.path.exists( self.base_path ):
			raise Exception( f"Required directory {self.base_path} does not exist..." )
		
		# Move to the base directory.
		os.chdir( f"{self.base_path}/" )

		# <=======================================================> #
		"""
		Load the required info on memory.
		Create plots for lengths and contact count distributions.
		See self.module1().
		"""
		print( "Loading required binary complexes on memory...\n" )

		self.module1()

		print( "Merged binary complexes selected = ", self.logger["counts"]["selected_binary_complexes"] )

		print( "\n------------------------------------------------------------------\n" )
		# <=======================================================> #
		"""
		Now we need to redundancy reduce our dataset at the following levels:
			Create an Out of Distribution (OOD) test set which:
				Is non-redundant wrt to self.
				Is non-redundant wrt our train set.
				Is non-redundant wrt AF2 PDB70 dataset.
			Create a train set which:
				Has no entry that belongs to the OOD set.
				Is non-redundant wrt to self.
		First creating the OOD test set.
		"""
		if os.path.exists( self.test_keys_file ):
			# Load the logger state.
			with open( self.logger_file, "r" ) as f:
				self.logger = json.load( f )

			print( "\nTotal non-redundant Uniprot ID pairs: ", self.logger["counts"]["seq_nr_uni_pairs"] )
			# print( "\nUniprot ID pairs seq non-redundant with PDB70: ", self.logger["counts"]["pdb70_nr_ood_uni_pairs"] )
			print( "Test set keys obtained = ", self.logger["counts"]["test_keys"] )
		
		else:
			tic = time.time()
			# for key in ["not_aligned"]:
			# 	self.logger[key] = [[], 0]
			print( "Creating a non-redundant OOD set...\n" )
			self.module2()
			toc = time.time()
			time1 = toc - tic
			self.logger["time_taken"]["ood_set_creation"] = time1
			print( "\nTotal non-redundant Uniprot ID pairs: ", self.logger["counts"]["seq_nr_uni_pairs"] )
			print( "Test set keys obtained = ", self.logger["counts"]["test_keys"] )
			print( f"Time taken to create the OOD test set = {time1/60} minutes..." )

			# Save the state.
			with open( self.logger_file, "w" ) as w:
				json.dump( self.logger, w )

		print( "\n------------------------------------------------------------------\n" )
		# <=======================================================> #
		"""
		Create a non-redundant train set.
		"""
		if os.path.exists( self.train_keys_file ):
			# Load the logger state.
			with open( self.logger_file, "r" ) as f:
				self.logger = json.load( f )

			print( "Train set keys obtained = ", self.logger["counts"]["train_uni_pairs"] )

		else:
			tic = time.time()
			print( "Creating a non-redundant train set...\n" )
			self.module3()
			toc = time.time()
			time2 = toc - tic
			self.logger["time_taken"]["train_set_creation"] = time2
			print( f"Time taken to create train set = {time2/60} minutes..." )
			print( "Train set keys obtained = ", self.logger["counts"]["train_uni_pairs"] )

			# Save the state.
			with open( self.logger_file, "w" ) as w:
				json.dump( self.logger, w )

		print( "\n------------------------------------------------------------------\n" )
		# <=======================================================> #
		"""
		Separate train and test set entries and create respective FASTA files.
		"""
		print( "Segregating train and test sets..." )
		self.module4()

		print( "\n------------------------------------------------------------------\n" )
		# <=======================================================> #
		"""
		Saving the dataset and logs.
		"""
		print( "Saving dataset and logs..." )
		self.save_dataset()



###################################################################################################################
###################################################################################################################
###################################################################################################################
	def get_pdb_from_entrykey( self, merged_entry_keys ):
		"""
		Extract the PDB ID from the entry_key.
			entry_key = "{entry_id}_{entry_idx}_{pdb}:{chain_id1}:{chain_id2}"
		merged_entry_keys --> list entry_keys for all the merged entries.

		Input:
		----------
		merged_entry_keys --> list of entry_keys for 
					merged binary complexes for a Uniprot ID pair.
			e.g. {UniID1}--{UniID2}_{copy_num}_{index}_{PDB ID}:{Chain1}:{Chain2}

		Returns:
		----------
		all_pdbs --> list of all PDB IDs extracted from the entry_keys.
		"""
		all_pdbs = [key.split( "_" )[-1].split( ":" )[0] for key in merged_entry_keys]

		return all_pdbs


	def plot_dist( self, counts, file_name ):
		"""
		Create plots for:
			Distribution of prot1 and prot2 lengths.
			Distribution of no. of contacts in each merged binary complex.

		Input:
		----------
		counts --> list of contact counts across all merged contact maps.
		file_name --> name for the output plot to be saved.

		Returns:
		----------
		None
		"""
		fig, axis = plt.subplots( 1, 3, figsize = ( 20, 15 ) )
		axis[0].hist( counts[0], bins = 20 )
		axis[1].hist( counts[1], bins = 20 )
		axis[2].hist( counts[2], bins = 20 )
		axis[0].xaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[1].xaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[2].xaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[0].yaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[1].yaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[2].yaxis.set_tick_params( labelsize = 14, length = 8, width = 2 )
		axis[0].set_ylabel( "Counts", fontsize = 16 )
		axis[1].set_ylabel( "Counts", fontsize = 16 )
		axis[2].set_ylabel( "Counts", fontsize = 16 )
		axis[0].set_title( "Contacts distribution", fontsize = 16 )
		axis[1].set_title( "Prot1 lengths distribution", fontsize = 16 )
		axis[2].set_title( "Prot2 lengths distribution", fontsize = 16 )

		plt.savefig( f"./Summary_plots_{file_name}.png", dpi = 300 )



	def module1( self ):
		"""
		Load the protein sequences for all merged binary complexes on memory.
		We filter binary complexes based on the fraction of contacts.
			This helps us reduce some of the sparsity.
		For each binary complexes we need to get the unique PDB IDs that were merged.
		Create plots for:
			Distribution of prot1 and prot2 lengths.
			Distribution of no. of contacts in each merged binary complex.
				Plots are created before and after selection based on fraction of contacts.
		Save accessory info. as a json file.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		with open( self.merged_binary_complexes_file, "r" ) as f:
			merged_binary_complexes = f.readlines()[0].split( "," )
		
		print( "Total merged binary complexes obtained = ", len( merged_binary_complexes ) )
		
		counts1 = [[], [], []]
		counts2 = [[], [], []]
		for entry_id in merged_binary_complexes:
			hf = h5py.File( f"{self.merged_binary_complexes_dir}{entry_id}.h5", 'r' )
			
			count = int( np.array( hf["contacts_count"] ) )
			total = np.array( hf["binary_cmap"] ).size
			
			len1 = int( np.array( hf["prot1_length"] ) )
			len2 = int( np.array( hf["prot2_length"] ) )

			counts1[0].append( count )
			counts1[1].append( len1 )
			counts1[2].append( len2 )

			merged_entry_keys = [e.decode( "utf-8" ) for e in hf["merged_entries"] ]
			self.unique_merged_pdbs.update( self.get_pdb_from_entrykey( merged_entry_keys ) )

			if count/total > self.contact_range[0] and count/total < self.contact_range[1]:
				counts2[0].append( count )
				counts2[1].append( len1 )
				counts2[2].append( len2 )

				self.entry_dict[entry_id] = {}

				prot1_seq = "".join( [e.decode( "utf-8" ) for e in hf["prot1_seq"]] )
				prot2_seq = "".join( [e.decode( "utf-8" ) for e in hf["prot2_seq"]] )
				self.entry_dict[entry_id]["prot1_seq"] = prot1_seq
				self.entry_dict[entry_id]["prot1_uni_boundary"] = hf["prot1_uni_boundary"][()].decode( "utf-8" )
				self.entry_dict[entry_id]["prot2_seq"] = prot2_seq
				self.entry_dict[entry_id]["prot2_uni_boundary"] = hf["prot2_uni_boundary"][()].decode( "utf-8" )
				self.entry_dict[entry_id]["merged_pdbs"] = set( merged_entry_keys )
		
		self.plot_dist( counts1, "pre_filter" )
		self.plot_dist( counts2, "post_filter" )

		self.logger["counts"]["selected_binary_complexes"] = len( self.entry_dict )
		print( f"Complexes obtained with >{self.contact_range[0]} and < {self.contact_range[1]} contacts = {len( self.entry_dict )}" )


###################################################################################################################
###################################################################################################################
###################################################################################################################
	def get_difference_PDB_set( self ):
		"""
		Get all PDBs not included in AF2 PDB70 set.
		Read the AF2 PDB70 *_clu.tsv file.
			Column0 contains the cluster representative PDB IDs.
			Column1 contains all PDB IDs.
			All PDB IDs need to be converted to lowercase before proceeding.
		Get the difference set using set operations.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		difference_set --> list of all PDB IDs not present in PDB70.
		"""
		df = read_mmseqs_tsv_output( self.pdb70_tsv_file )
		all_af2_pdbs = set( df[1].str.split( "_" ).str[0].str.lower() )

		difference_set = list( self.unique_merged_pdbs - all_af2_pdbs )

		return difference_set


	def get_prot_header( self, entry_id ):
		"""
		Create a header for prot1/2 sequence to uniquely identify them.
		This is required to identify the sequences post clustering.
		header = "{entry_id}:{uni_id1/2}-{bound1/2}"
			Note that the purpose for header is different from entry_id.
				entry_id is used to uniquely identify the Uniprot ID pair 
				(representing a merged binary complex) while the header 
				uniquely identifies a seq within a pair.

		Input:
		----------
		entry_id --> identifier for each merged binary complex.
				"{Uni_ID1}--{Uni_ID2}_{copy_num}"

		Returns:
		----------
		prot1_header --> (str) "{entry_id}:{uni_id1}-{bound1}"
		prot2_header --> (str) "{entry_id}:{uni_id2}-{bound2}"
		"""
		uni1, uni2 = entry_id.split( "--" )
		uni2, _ = uni2.split( "_" )
		bound1 = self.entry_dict[entry_id]["prot1_uni_boundary"]
		bound2 = self.entry_dict[entry_id]["prot2_uni_boundary"]

		prot1_header = f"{entry_id}:{uni1}-{bound1}"
		prot2_header = f"{entry_id}:{uni2}-{bound2}"

		return prot1_header, prot2_header



	def create_fasta_for_mseqs2( self, entry_ids, fasta_file, pdb70 = False ):
		"""
		Create a FASTA file containing the seqs for all prot1 and prot2.
			>header
			Sequence
		We use a header to distinguish prot1/2 seq for the same entry_id.
			See self.get_prot_header()

		Input:
		----------
		entry_ids --> list of entry_id whose prot1/2 
			sequences are to be included in the FASTA file.
		fasta_file --> name of fasta file.
		pdb70 --> (bool) if True adds PDB70 sequences to FASTA file.
			Required for OOD set creation.

		Returns:
		----------
		None
		"""
		if pdb70:
			with open( f"../{self.pdb70_rep_fasta_file}", "r" ) as f:
				pdb70_fasta = f.readlines()

		with open( fasta_file, "w" ) as w:
			for entry_id in entry_ids:
				# Headers for prot1/2 are needed to uniquely identify them post clustering.
				prot1_header, prot2_header = self.get_prot_header( entry_id )
				
				w.writelines( f">{prot1_header}\n{self.entry_dict[entry_id]['prot1_seq']}\n\n" )
				w.writelines( f">{prot2_header}\n{self.entry_dict[entry_id]['prot2_seq']}\n\n" )
			
			if pdb70:
				# Add all PDB70 sequences.
				w.writelines( pdb70_fasta )


	def get_singleton_doublet_clusters( self, tsv_file ):
		"""
		From the clusters generated by MMSeqs2 at --min-seq-id,
			obtain the ones which:
			Do not contain a PDB70 chain (identified by 'AF_' prefix).
			Are singleton (contain only 1 sequence i.e. self).
			Are doublet (contain only two sequences).

		Input:
		----------
		tsv_file --> MMSeqs2 generated cluster tsv file.

		Returns:
		----------
		singleton_clusters --> list of all headers that are part of a singleton cluster.
		doublet_clusters --> list of all headers that are part of a doublet cluster.
		"""
		singleton_clusters = []
		doublet_clusters = []
		df = read_mmseqs_tsv_output( tsv_file )

		for grp in df.groupby( by = 0 ):
			if not grp[1][1].str.contains( "AF_" ).any():
				if len( grp[1][1] )== 1:
					singleton_clusters.extend( grp[1][1].tolist() )
				elif len( grp[1][1] ) == 2:
					doublet_clusters.append( grp[1][1].tolist() )

		return singleton_clusters, doublet_clusters


	def get_seq_nr_ood_pairs( self, entry_id ):
		"""
		Check if all the merged PDB IDs for the entry are 
			absent from the difference set.
		Consider sequence redundant with the entire dataset and PDB70 if:
			Both Uniprot IDs belong to a singleton cluster.
							OR
			Both Uniprot IDs belong to a doublet cluster.
		
		entry_id --> identifier for each merged binary complex.
				"{Uni_ID1}--{Uni_ID2}_{copy_num}"
		Input:
		----------
		entry_id --> identifier for each merged binary complex.
				"{Uni_ID1}--{Uni_ID2}_{copy_num}"

		Returns:
		----------
		success --> (bool) True if both checks passed.
		entry_id --> same as input.
		"""
		merged_pdbs = [pdb.split( "_" )[-1].split( ":" )[0] for pdb in self.entry_dict[entry_id]["merged_pdbs"]]
		# Remove the chain IDs from the PDB IDs.
		# e.g. 1XXX:A:B --> 1XXX
		pdb_ids = [id_.split( ":" )[0] for id_ in merged_pdbs]
		# merged_pdbs = list( pd.unique( np.array( merged_pdbs ) ) )
		chk1, chk2 = False, False

		prot1_header, prot2_header = self.get_prot_header( entry_id )

		chk1 = all( [pdb in self.difference_set for pdb in pdb_ids] )
		if prot1_header in self.singleton_clusters and prot2_header in self.singleton_clusters:
			chk2 = True
		elif [prot1_header, prot2_header] in self.doublet_clusters:
			chk2 = True

		success = all( [chk1, chk2] )

		return [success, entry_id]



	def module2( self ):
		"""
		Create a Out of Distribution (OOD) test set.
		Requirements for the ODO set:
			1. No overlapping PDB with AF2 PDB70.
				Create a difference set of PDBs that are not present in PDB70.
				Consider Uniprot ID pairs for which all merged PDBs belong to the difference set.
				This acts as a low pass filter to reduce computational expense downstream.
			2. Sequence based non-redundancy check.
				<20% all-vs-all sequence identity.
				Use MMSeqs2 for clustering all prot1/2 sequence at --min-seq-id.
				Consider Uniprot ID pairs for which uni_id1 and uni_id2 are singleton clusters 
					or both are part of a doublet cluster.
			3. Structure based non-redundancy check (computationally expensive).
				Instead using only sequence based non-redundancy check with PDB70.
				Consider Uniprot ID pairs for which both (or one of the two) 
					Uniprot IDs do not cluster with any PDB70 chain.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		# Create a difference set and get all singleton clusters.
		# ---------------------------------------------------------------
		if not os.path.exists( self.ood_mm_dir ):
			os.makedirs( self.ood_mm_dir )
		os.chdir( self.ood_mm_dir )
		
		self.create_fasta_for_mseqs2( self.entry_dict.keys(), self.ood_fasta_file )
		mmseqs_cluster( self.ood_fasta_file, self.name2, 
						self.mmseqs_algo, self.ood_ID_cutoff, self.mmseqs_cluster_mode )

		os.chdir( "../" )

		self.difference_set = self.get_difference_PDB_set()
		self.singleton_clusters, self.doublet_clusters = self.get_singleton_doublet_clusters( tsv_file = self.ood_tsv_file )

		with open( self.difference_set_file, "w" ) as w:
			w.writelines( ",".join( self.difference_set ) )

		self.logger["counts"]["difference_set"] = len( self.difference_set )
		self.logger["counts"]["singleton_clusters"] = len( self.singleton_clusters )
		self.logger["counts"]["doublet_clusters"] = len( self.doublet_clusters )
		print( "Difference set: ", self.logger["counts"]["difference_set"] )
		print( "All Singleton clusters excluding PDB70 clusters: ", self.logger["counts"]["singleton_clusters"] )
		print( "All Doublet clusters excluding PDB70 clusters: ", self.logger["counts"]["doublet_clusters"] )

		# Select Uniprot ID pairs that are sequence non-redundant.
		# ---------------------------------------------------------------
		print( "\n------------------------------------------------------------------" )
		print( "Selecting Uniprot ID pairs that are non-redundant at sequence level..." )
		seq_nr_uni_pairs = []
		unique_merged_pdbs = set()
		with Pool( self.cores ) as p:
			for result in tqdm.tqdm( 
								p.imap_unordered( 
												self.get_seq_nr_ood_pairs, self.entry_dict.keys() ), 
						total = len( self.entry_dict.keys() ) ):
				passed, entry_id = result

				if passed:
					seq_nr_uni_pairs.append( entry_id )
					unique_merged_pdbs.update( 
									[key.split( "_" )[-1] for key in self.entry_dict[entry_id]["merged_pdbs"]]
											 )
		
		self.logger["counts"]["seq_nr_merged_pdbs"] = len( unique_merged_pdbs )
		self.logger["counts"]["seq_nr_uni_pairs"] = len( seq_nr_uni_pairs )
		print( "\nSeq non-redundant Uni ID pairs obtained: ", self.logger["counts"]["seq_nr_uni_pairs"] )
		print( "\nMerged PDBs for all seq non-redundant Uni ID pairs: ", self.logger["counts"]["seq_nr_merged_pdbs"] )


		# Of all the non-redundant binary complexes obtained select a 
		# 		subset as OOD entries.
		# ---------------------------------------------------------------
		print( "\n------------------------------------------------------------------" )
		print( "Selecting OOD keys..." )
		total = len( self.entry_dict )

		# Sample from all the sequence obtained with <20 % identity.
		if len( seq_nr_uni_pairs ) <= self.ood_fraction*total:
			print( "Taking all the entries for test set..." )
			test_keys = seq_nr_uni_pairs
		
		else:
			test_keys = np.random.choice( 
							list( seq_nr_uni_pairs ), math.ceil( self.ood_fraction*total ), 
							replace = False )

		self.logger["counts"]["test_keys"] = len( test_keys )
		with open( self.test_keys_file, "w" ) as w:
			w.writelines( ",".join( test_keys ) )



###################################################################################################################
###################################################################################################################
###################################################################################################################
	def get_representative_for_member( self, df, prot1_header, prot2_header ):
		"""
		Given the header for prot1/2, identify the cluster 
			representative for them.

		Input:
		----------
		prot1_header --> (str) header for prot1.
		prot1_header --> (str) header for prot2.
		See self.get_prot_header() header info.
		
		Returns:
		----------
		prot1_rep --> cluster representative for prot1.
		prot2_rep --> cluster representative for prot2.
		"""
		ind1 = df.index[df[1] == prot1_header].tolist()
		ind2 = df.index[df[1] == prot2_header].tolist()
		prot1_rep = df.iloc[ind1[0], 0]
		prot2_rep = df.iloc[ind2[0], 0]

		return prot1_rep, prot2_rep



	def get_seq_nr_train_pairs( self, non_ood_keys ):
		"""
		We consider a Uniprot ID pair for train set if:
			Both prot1/2 headers are cluster representatives.
							OR
			Both prot1/2 headers belong to the same cluster.

		Input:
		----------
		non_ood_keys --> list of entry_ids after separating out OOD entry_ids.
		
		Returns:
		----------		
		train_keys --> list of entry_id selected for non-redundant train set.
		"""
		train_keys = []
		df = read_mmseqs_tsv_output( self.train_tsv_file )

		representatives = df[0].tolist()

		for entry_id in non_ood_keys:
			prot1_header, prot2_header = self.get_prot_header( entry_id )
			prot1_rep, prot2_rep = self.get_representative_for_member( df, prot1_header, prot2_header )
			
			if prot1_header in representatives and prot2_header in representatives:
				train_keys.append( entry_id )
			elif prot1_rep == prot2_rep:
				train_keys.append( entry_id )

		return train_keys


	def module3( self ):
		"""
		Create training set for downstream model development.
		Remove the test keys from training set.
		This includes all binary complexes that have not been selected for the test set.
		if self.redundancyreduce_train
			Redundancy reduce at 40% sequence identity.
			Select entry_id as specified in self.get_seq_nr_train_pairs().
		else:
			Take all entry_id not part of OOD set for training.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		print( "\n------------------------------------------------------------------" )
		print( "Redundancy reducing training set..." )
		with open( self.test_keys_file, "r" ) as f:
			test_keys = f.readlines()[0].split( "," )

		non_ood_keys = [key for key in self.entry_dict.keys() if key not in test_keys]

		# Redundancy reduce training set if specified.
		if self.redundancyreduce_train:
			if not os.path.exists( self.train_mm_dir ):
				os.makedirs( self.train_mm_dir )
			os.chdir( self.train_mm_dir )
			
			self.create_fasta_for_mseqs2( non_ood_keys, self.train_fasta_file )
			mmseqs_cluster( self.train_fasta_file, self.name3, 
							self.mmseqs_algo, self.train_ID_cutoff, self.mmseqs_cluster_mode )

			os.chdir( "../" )

			# Consider Uniprot ID pairs for which both proteins are cluster representatives..
			# ---------------------------------------------------------------
			train_keys = self.get_seq_nr_train_pairs( non_ood_keys )
		
		# Take all non-OOD set keys as training set keys.
		else:
			train_keys = non_ood_keys
		
		self.logger["counts"]["train_uni_pairs"] = len( train_keys )
		print( "\nUni ID pairs obtained for train set: ", self.logger["counts"]["train_uni_pairs"] )

		with open( self.train_keys_file, "w" ) as w:
			w.writelines( ",".join( train_keys ) )



###################################################################################################################
###################################################################################################################
###################################################################################################################
	def create_af2_input( self ):
		"""
		Create a directory containng fasta files to be used as input by AF2.
			Each file contains Uniprot seq for the protein pairs with 
				the respective Uniprot IDs as header.
			e.g. "{entry}.fasta" 
				>"{Uni_ID1}:start:end"
				Sequence for prot1

				>"{Uni_ID2}:start:end"
				Sequence for prot2
			start, end refer to the the first and last residue positions.
		Also dump the paths to all FASTA files in a comma-separated txt file 
			for ease of running AF2.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		if not os.path.exists( self.af2_fasta_dir ): 
			os.makedirs( f"{self.af2_fasta_dir}" )

		w_path =  open( self.af2_input_file, "w" )
		with open( self.af2_input_file, "w" ) as w:
			for entry_id in self.test_dict.keys():
				head1, head2 = entry_id.split( "--" )
				head2 = head2.split( "_" )[0]

				bound1 = self.test_dict[entry_id]["prot1_uni_boundary"].split( "-" )
				bound2 = self.test_dict[entry_id]["prot2_uni_boundary"].split( "-" )

				head1 += f":{bound1[0]}:{bound1[1]}"
				head2 += f":{bound2[0]}:{bound2[1]}"

				with open( f"{self.af2_fasta_dir}{entry_id}.fasta", "w" ) as w:
					w.writelines( ">" + head1 + "\n" + self.test_dict[entry_id]["prot1_seq"] + "\n\n" )
					w.writelines( ">" + head2 + "\n" + self.test_dict[entry_id]["prot2_seq"] + "\n" )

				w_path.writelines( f"{self.af2_fasta_dir}{entry_id}.fasta," )
		w_path.close()



	def create_af3_input( self ):
		"""
		Create JSON file for batch running AF3 server.
		Will create batches of 20 merged binary complexes.
		AF3 requires a list of dict in JSON format 
				(https://github.com/google-deepmind/alphafold/blob/main/server/README.md).
		The entry_id serves as the job name.
		We don't specify PRNG seeds.
		This allows to upload 20 jobs on the AF3 server at once, 
				but you still need to run each job one by one.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		if not os.path.exists( self.af3_json_dir ): 
			os.makedirs( f"{self.af3_json_dir}" )

		test_entry_ids = list( self.test_dict.keys() )
		for start in np.arange( 0, len( test_entry_ids ), 20 ):
			if ( start + 20 ) > len( test_entry_ids ):
				end = len( test_entry_ids )
			else:
				end = start + 20

			for entry_id in test_entry_ids[start:end]:
				af3_batch = []
				af3_entry = {}
				af3_entry["name"] = entry_id
				af3_entry["modelSeeds"] = []
				af3_entry["sequences"] = [
									{
									"proteinChain": {

											"sequence": self.test_dict[entry_id]["prot1_seq"],
											"count": 1
									},
									"proteinChain": {

											"sequence": self.test_dict[entry_id]["prot2_seq"],
											"count": 1
									}
									}
				]

				af3_batch.append( af3_entry )

			# Save batches of 20 merged binary complexes.
			with open( f"{self.af3_json_dir}Batch_{start}-{end}.json", "w" ) as w:
				json.dump( af3_batch, w )



	def module4( self ):
		"""
		Create input files for AF2 and AF3:
			 FASTA files for AF2 and txt file containing path to each file.
			 JSON files contaiing batches of 20 entries for AF3.
		Create dict for storing test and train set data.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		with open( self.test_keys_file, "r" ) as f:
			test_keys = f.readlines()[0].split( "," )

		with open( self.train_keys_file, "r" ) as f:
			train_keys = f.readlines()[0].split( "," )

		for entry_id in self.entry_dict.keys():
			hf = h5py.File( f"{self.merged_binary_complexes_dir}{entry_id}.h5", 'r' )
			self.entry_dict[entry_id]["binary_cmap"] = np.array( hf["binary_cmap"] )

		# Select all entries for OOD test set.
		for entry_id in test_keys:
			self.test_dict[entry_id] = {}
			for key in self.entry_dict[entry_id].keys():
				self.test_dict[entry_id][key] = self.entry_dict[entry_id][key]

		# Select all entries for train set.
		for entry_id in train_keys:
			self.train_dict[entry_id] = {}
			for key in self.entry_dict[entry_id].keys():
				self.train_dict[entry_id][key] = self.entry_dict[entry_id][key]

		del self.entry_dict
		# Create input FASTA files for AF2.
		self.create_af2_input()
		# Create batches in JSON format for AF3.
		self.create_af3_input()


###################################################################################################################
###################################################################################################################
###################################################################################################################
	def save_dataset( self ):
		"""
		Save the train and test datasets:
			Fasta files for prot1 and prot2.
				Fasta Header is a modified entry_id which includes UniprotID and 
												the boundaries for both proteins.
				e.g. entry_id --> UniID1--UniID2_num 
					 Fasta header --> UniID1:start1:end1--UniID2:start2:end2_num
			 	This format would be required to create embeddings downstream.
		 	csv file containing Fasta header.
			Binary contact map, 1 hot encodings for both proteins as h5 files.

		Input:
		----------
		Does not take any input arguments.

		Returns:
		----------
		None
		"""
		for set_ in zip( ["train", "test"], [self.train_dict, self.test_dict] ):
			name, set_ = set_
		
			print( f"Saving the {name} dataset..." )
			prot1, prot2 = {}, {}
			# w1 = open( f"prot1_{name}_fasta_{version}.fasta", "w" )
			# w2 = open( f"prot2_{name}_fasta_{version}.fasta", "w" )
			w = open( f"prot_1-2_{name}_{version}.csv", "w" )

			hf1 = h5py.File( f"Output_bcmap_{name}_{version}.h5", "w" )

			# Save the sequence and cmaps.
			for key in set_.keys():
				head1, head2 = key.split( "--" )
				head2, num = head2.split( "_" )
				bound1 = set_[key]["prot1_uni_boundary"].split( "-" )
				bound2 = set_[key]["prot2_uni_boundary"].split( "-" )

				head1 += f":{bound1[0]}:{bound1[1]}"
				head2 += f":{bound2[0]}:{bound2[1]}"
				head = f"{head1}--{head2}_{num}"
				w.writelines( f"{head}," )

				hf1.create_dataset( head, data = set_[key]["binary_cmap"] )
			w.close()
			hf1.close()

		self.tom = time.time()
		self.logger["time_taken"]["total"] = self.tom - self.tim
		print( "Total time taken = %s minutes\n "%( self.logger["time_taken"]["total"]/60 ) )

		proc = subprocess.Popen( "hostname", shell = True, stdout = subprocess.PIPE, )
		system = proc.communicate()[0]
		proc = subprocess.Popen( "date", shell = True, stdout = subprocess.PIPE )
		sys_date = proc.communicate()[0]

		# Create the log file.
		with open( f"Logs_OOD{self.version}.txt", "w" ) as w:
			w.writelines( "<------------------------Logs for Disobind Dataset------------------------>\n\n" )
			w.writelines( f"System = {system.strip()} \n" )
			w.writelines( f"Date = {sys_date} \n" )

			w.writelines( "\n---------------------------------------------------------\n" )
			w.writelines( "------------------Settings used\n" )
			w.writelines( f"No. of cores = {self.cores}\n" )
			w.writelines( f"Min-Max contacts fraction = {self.contact_range[0]}-{self.contact_range[1]}\n" )
			w.writelines( f"Redundancy reduce the train set = {self.redundancyreduce_train}\n" )
			w.writelines( f"Train min-seq-id = {self.train_ID_cutoff}\n" )
			# w.writelines( f"Redundancy reduce both seq for OOD set = {self.redundancyreduce_both}\n" )
			w.writelines( f"OOD min-seq-id = {self.ood_ID_cutoff}\n" )
			w.writelines( f"Test set fraction = {self.ood_fraction}\n" )
			w.writelines( f"TM score cutoff = {self.tm_cutoff}\n" )
			w.writelines( f"MMSeqs2 args: Algorithm = {self.mmseqs_algo} \t Cluster mode = {self.mmseqs_cluster_mode}\n" )
			# w.writelines( f"USalign args: mol = {self.mol} \t ter = {self.ter}\n" )

			w.writelines( "\n---------------------------------------------------------\n" )
			w.writelines( "------------------Time taken\n" )
			w.writelines( "Creating OOD set: %s minutes\n"%( self.logger["time_taken"]["ood_set_creation"]/60 ) )
			w.writelines( "Creating Train set: %s minutes\n"%( self.logger["time_taken"]["train_set_creation"]/60 ) )
			w.writelines( "Total: %s minutes\n"%( self.logger["time_taken"]["total"]/60 ) )

			
			w.writelines( "\n---------------------------------------------------------\n" )
			w.writelines( "------------------Counts\n" )
			w.writelines( "Selected binary complexes: %s\n"%self.logger["counts"]["selected_binary_complexes"] )
			w.writelines( "PDBs in the difference set: %s\n"%self.logger["counts"]["difference_set"] )
			w.writelines( "Singleton clusters: %s\n"%self.logger["counts"]["singleton_clusters"] )
			w.writelines( "Doublet clusters: %s\n"%self.logger["counts"]["doublet_clusters"] )
			w.writelines( "Seq non-redundant binary complexes: %s\n"%self.logger["counts"]["seq_nr_uni_pairs"] )
			w.writelines( "Total PDBs for seq non-redundant binary complexes: %s\n"%self.logger["counts"]["seq_nr_merged_pdbs"] )
			w.writelines( "Total Test set entries: %s\n"%self.logger["counts"]["test_keys"] )
			w.writelines( "Total Train set entries: %s\n"%self.logger["counts"]["train_uni_pairs"] )
			w.writelines( "\n---------------------------------------------------------\n" )

			for key in self.logger.keys():
				w.writelines( "-->" + key + "\n" )
				if key not in ["time_taken", "counts"]:
					w.writelines( "\nCount = " + str( self.logger[key][1] ) + "\n" )
					w.writelines( "========================================================\n" )

		# Save state.
		with open( self.logger_file, "w" ) as w:
			json.dump( self.logger, w )



if __name__ == "__main__":
	parser = argparse.ArgumentParser( description="Create Disobind dataset" )
	parser.add_argument( '--version', '-v', dest = "v", help = "Version of the dataset", required = True, type = str )
	parser.add_argument( '--max_cores', '-c', dest="c", help = "No. of cores to be used.", type = int, required=False, default=10 )
	
	version = parser.parse_args().v
	cores = parser.parse_args().c
	
	NonRedundantDataset( version, cores ).forward()
	print( "May the Force b with u..." )
